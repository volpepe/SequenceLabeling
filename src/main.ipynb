{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import zipfile\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import datetime\n",
    "from IPython.display import display\n",
    "\n",
    "from typing import List, Dict, Tuple, Sequence\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "import gensim\n",
    "import gensim.downloader as gloader\n",
    "assert int(gensim.__version__.split('.')[0]) >= 4, \"Install gensim 4.x.x or above (pip install -U gensim)\"\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "BATCH_SIZE = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_PATH = os.path.dirname(os.getcwd())\n",
    "DATA_PATH = os.path.join(ROOT_PATH, 'data')\n",
    "\n",
    "def save_response_content(response, destination):\n",
    "    CHUNK_SIZE = 32768\n",
    "\n",
    "    with open(destination, \"wb\") as f:\n",
    "        for chunk in response.iter_content(CHUNK_SIZE):\n",
    "            if chunk: # filter out keep-alive new chunks\n",
    "                f.write(chunk)\n",
    "\n",
    "def download_data(data_path):\n",
    "    final_path = os.path.join(data_path, 'dependency_treebank.zip')\n",
    "    url = \"https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/packages/corpora/dependency_treebank.zip\"\n",
    "\n",
    "    if not os.path.exists(data_path):\n",
    "        os.makedirs(data_path)\n",
    "\n",
    "    if not os.path.exists(final_path):\n",
    "        print(\"Downloading dataset...\")\n",
    "        with requests.Session() as current_session:\n",
    "            response = current_session.get(url, stream=True)\n",
    "        save_response_content(response, final_path)\n",
    "        print(\"Download completed!\")\n",
    "\n",
    "        print(\"Extracting dataset...\")\n",
    "        with zipfile.ZipFile(final_path) as loaded_zip:\n",
    "            loaded_zip.extractall(data_path)\n",
    "        print(\"Extraction completed!\")\n",
    "\n",
    "download_data(DATA_PATH)\n",
    "\n",
    "# The new data path is:\n",
    "DATA_PATH = os.path.join(DATA_PATH, 'dependency_treebank')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is composed of several files whose naming convention is `wsj_{num}.dp`. We explore the content of the first file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Pierre\\tNNP\\t2', 'Vinken\\tNNP\\t8', ',\\t,\\t2', '61\\tCD\\t5', 'years\\tNNS\\t6', 'old\\tJJ\\t2', ',\\t,\\t2', 'will\\tMD\\t0', 'join\\tVB\\t8', 'the\\tDT\\t11', 'board\\tNN\\t9', 'as\\tIN\\t9', 'a\\tDT\\t15', 'nonexecutive\\tJJ\\t15', 'director\\tNN\\t12', 'Nov.\\tNNP\\t9', '29\\tCD\\t16', '.\\t.\\t8', '', 'Mr.\\tNNP\\t2', 'Vinken\\tNNP\\t3', 'is\\tVBZ\\t0', 'chairman\\tNN\\t3', 'of\\tIN\\t4', 'Elsevier\\tNNP\\t7', 'N.V.\\tNNP\\t12', ',\\t,\\t12', 'the\\tDT\\t12', 'Dutch\\tNNP\\t12', 'publishing\\tVBG\\t12', 'group\\tNN\\t5', '.\\t.\\t3']\n"
     ]
    }
   ],
   "source": [
    "with open(os.path.join(DATA_PATH, 'wsj_0001.dp'), 'r') as f:\n",
    "    lines = [l.rstrip() for l in f.readlines()]\n",
    "\n",
    "print(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each file contains a paragraph of text, optionally composed of multiple sentences. \n",
    "\n",
    "The paragraph is structured as a sequence of lines, each line containing a word, a tag and a number separated by `\\t` tags. We don't care about the number, so we ignore the third element.\n",
    "\n",
    "We can define a function that given a file processes its content and returns a Pandas `DataFrame` for each sentence extracted from the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Pierre</td>\n",
       "      <td>NNP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Vinken</td>\n",
       "      <td>NNP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>,</td>\n",
       "      <td>,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>61</td>\n",
       "      <td>CD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>years</td>\n",
       "      <td>NNS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>old</td>\n",
       "      <td>JJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>,</td>\n",
       "      <td>,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>will</td>\n",
       "      <td>MD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>join</td>\n",
       "      <td>VB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>the</td>\n",
       "      <td>DT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>board</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>as</td>\n",
       "      <td>IN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>a</td>\n",
       "      <td>DT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>nonexecutive</td>\n",
       "      <td>JJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>director</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Nov.</td>\n",
       "      <td>NNP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>29</td>\n",
       "      <td>CD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            word  tag\n",
       "0         Pierre  NNP\n",
       "1         Vinken  NNP\n",
       "2              ,    ,\n",
       "3             61   CD\n",
       "4          years  NNS\n",
       "5            old   JJ\n",
       "6              ,    ,\n",
       "7           will   MD\n",
       "8           join   VB\n",
       "9            the   DT\n",
       "10         board   NN\n",
       "11            as   IN\n",
       "12             a   DT\n",
       "13  nonexecutive   JJ\n",
       "14      director   NN\n",
       "15          Nov.  NNP\n",
       "16            29   CD\n",
       "17             .    ."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Mr.</td>\n",
       "      <td>NNP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Vinken</td>\n",
       "      <td>NNP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>is</td>\n",
       "      <td>VBZ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>chairman</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>of</td>\n",
       "      <td>IN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Elsevier</td>\n",
       "      <td>NNP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>N.V.</td>\n",
       "      <td>NNP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>,</td>\n",
       "      <td>,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>the</td>\n",
       "      <td>DT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Dutch</td>\n",
       "      <td>NNP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>publishing</td>\n",
       "      <td>VBG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>group</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          word  tag\n",
       "18         Mr.  NNP\n",
       "19      Vinken  NNP\n",
       "20          is  VBZ\n",
       "21    chairman   NN\n",
       "22          of   IN\n",
       "23    Elsevier  NNP\n",
       "24        N.V.  NNP\n",
       "25           ,    ,\n",
       "26         the   DT\n",
       "27       Dutch  NNP\n",
       "28  publishing  VBG\n",
       "29       group   NN\n",
       "30           .    ."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def process_file(filepath: str):\n",
    "    with open(filepath, 'r') as f:\n",
    "        # They are not technically csv files, but they are text files\n",
    "        # so we can still use the same function.\n",
    "        df = pd.read_csv(f, sep='\\t', names=['word','tag','drop'])\n",
    "    # Drop the last column\n",
    "    df = df.drop(['drop'], axis=1)\n",
    "    # Find indices of the dataframe containing points, \n",
    "    # meaning we have found the beginning of a new sentence.\n",
    "    points_indices = df.index[df['word'] == '.']\n",
    "    \n",
    "    # Split dataframes based on those indices\n",
    "    sentences = []\n",
    "    last_index = 0\n",
    "    for idx in points_indices:\n",
    "        sentences.append(df.iloc[last_index:idx+1])\n",
    "        last_index = idx+1\n",
    "    return sentences\n",
    "\n",
    "# Example:\n",
    "test_sentences = process_file(os.path.join(DATA_PATH, 'wsj_0001.dp'))\n",
    "for sentence in test_sentences:\n",
    "    display(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are required to split the dataset using:\n",
    "- Documents 1-100 as the train set.\n",
    "- Documents 101-150 as the validation set.\n",
    "- Documents 151-199 as the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we compute all filenames for the splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_filenames = [ os.path.join(DATA_PATH, 'wsj_{:04d}.dp'.format(x)) for x in range(1  , 101) ] \n",
    "val_filenames   = [ os.path.join(DATA_PATH, 'wsj_{:04d}.dp'.format(x)) for x in range(101, 151) ]\n",
    "test_filenames  = [ os.path.join(DATA_PATH, 'wsj_{:04d}.dp'.format(x)) for x in range(151, 200) ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we translate the filenames into dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences = [ sentence for file in train_filenames for sentence in process_file(file) ]\n",
    "val_sentences   = [ sentence for file in val_filenames   for sentence in process_file(file) ]\n",
    "test_sentences  = [ sentence for file in test_filenames  for sentence in process_file(file) ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define some utility functions that we can use to obtain sentences in other forms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Mr.', 'Vinken', 'is', 'chairman', 'of', 'Elsevier', 'N.V.', ',', 'the', 'Dutch', 'publishing', 'group', '.']\n",
      "Mr. Vinken is chairman of Elsevier N.V. , the Dutch publishing group .\n"
     ]
    }
   ],
   "source": [
    "def get_list_of_words_from_sentence(sentence: pd.DataFrame):\n",
    "    return list(sentence['word'])\n",
    "\n",
    "def get_natural_language_sentence(sentence):\n",
    "    if isinstance(sentence, pd.DataFrame):\n",
    "        sentence = get_list_of_words_from_sentence(sentence)\n",
    "    return \" \".join(sentence)\n",
    "\n",
    "sen_test = train_sentences[1]\n",
    "print(get_list_of_words_from_sentence(sen_test))\n",
    "print(get_natural_language_sentence(sen_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We study the distribution of the sentences and the tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1936 training sentences\n",
      "There are 1256 validation sentences\n",
      "There are 636 test sentences\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAj0AAAEICAYAAAC0zkWCAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAAd3klEQVR4nO3deZhlVXm28fuxGxkEGhAkDRLaAUGQQUSjUQxEogKJivoJCTEYp08TP4PGRCJqGoOKCZ9yEaeAAyQoGgURxQFNAEei3UwNKsrQBhkFGUVIA2/+2Kv0UNSpoenuqtP7/l3XuXrXHtZe796nqp5ae1V1qgpJkqS13UNmuwOSJElrgqFHkiT1gqFHkiT1gqFHkiT1gqFHkiT1gqFHkiT1gqFH0pyTZFGSSjK/ffzlJIesorb3THLpwMfLk+yzKtpu7V2SZK9V1d6akGSvJD+b7X7MxOB9S/KWJB+Z7T5p7jP0SHPMqvgmnORlSb61qvo026pq36o6car9WlB67BRtfbOqtl8V/UpyQpIjx7W/U1WdvSraX12mc50eRNt7J1mW5JYkNyX5XJKtV8e5xlTVu6rqle389wvM0iBDj6Te8BvhGvED4DlVtQmwFfAT4EOz2iOpMfRIc9jYiE2So5PcnOTKJPuO235FktvbtoOTPB74MPC0JHckuaXtu3+S85PcluSqJIsH2hn76fiQJP+d5MYkhw9sn9ceIVzezrU0yTZt2w5JvpbkF0kuTfKSgeP2S/KDdszVSd40pM55rcYbk1wB7D9u+9lJxn6Sf2ySc5Lc2vb/dFv/jbb7ha3uA8ce2yR5c5LrgI8PeZTz5NbPm5N8PMl6g9d/XF+q9eHVwMHA37bzfaFtH3zssm6SY5Jc017HJFm3bRvr218nuSHJtUn+fJL3wlZJTm/X+bIkrxrYtjjJvyf513atL0myx5B2HnCdBrZN2JdWx9HtvXF9kg8nWX+i9qvq+qq6ZmDVvcDQUaV2b65u/b40ybMGavpskk+3becl2XVIG4uTnNQ+HKvvllbf04a9Z9RDVeXLl6859AKWA/u05ZcBK4BXAfOA1wLXAAEeBtwGbN/2XQjsNHDct8a1uxewM90PO7sA1wMvaNsWAQUcD6wP7ArcDTy+bf8bYBmwfTv3rsDDWx+uAv4cmA/sDtw40I9rgT3b8qbA7kNqfg3wI2AbYDPgrNaf+W372cAr2/LJwOGtjvWAZwy0U8Bjx9V8D/AeYN1W217Az8Zd74sHzv1t4MhJruOvzwGcMLbvkPv3DuBc4BHAFsB3gH8Y17d3AOsA+wF3ApsOuUbnAB9sNe8G/Bx4Vtu2GLirtTEPeDdw7iTvsWHXacK+AMcAp7frsxHwBeDdk7T/28AtwH1079+XDdlv+/b+2WrgffiYgZpWAC9ufXoTcCWwzgTXeTFw0rj38vyB8wx9z/jq18uRHmnu+2lVHV9V9wIn0oWbLdu2+4AnJFm/qq6tqkuGNVJVZ1fVsqq6r6ouovtG8Hvjdjuiqn5VVRcCF9KFG4BXAm+tqkurc2FV3QT8IbC8qj5eVfdU1XnAKXTfqKD7prVjko2r6ua2fSIvAY6pqquq6hd037SHWQFsS/eN8q6qmmru0n3A31fV3VX1qyH7vH/g3O8E/niKNqfrYOAdVXVDVf0cOAJ46cD2FW37iqr6EnAHXRC4nzaq9gzgza3mC4CPjGvrW1X1pfY++Td+c++ma8K+JAld6H5DVf2iqm4H3gUcNKyhqvrv6h5vbQ68lS7QTuReujC6Y5J1qmp5VV0+sH1pVX22qlYA76ULLE+dYV1jtc3kPaO1lKFHmvuuG1uoqjvb4oZV9UvgQLpRkmuTnJFkh2GNJPmdJGcl+XmSW9txmw87F91P+hu25W2Ay3mgbYHfSTdp9ZZ0j9IOBn6rbX8R3ajBT9vjhacN6d5WdD/xj/npsDqAv6Ubbfpee4zz8kn2Bfh5Vd01xT7jz73VFPtP11bcv5bxbd9UVfcMfDx4zce3MxY4BtsanCA8/t6tl5nNYRrWly2ADYClA/f4K239pFqIPBH4/ER9qarLgEPpRmpuSPKpJIPX56qBfe8DfsbK3ZuZvme0ljL0SCOsqr5aVX9AN/rzI7rHU9AN74/3SbpHFNtU1QK6eT+Z5qmuAh4zZP05VbXJwGvDqnpt69/3q+r5dI93TgP+fUj719IFqzG/PawjVXVdVb2qqrYC/i/wwUz+m0gTXYvxxp97bE7KL+m+4QOQ5Le4v6navoYuGE7U9kxcA2yWZKNxbV29Em3N1I3Ar+geWY7d4wVVNVE4m8h8uvu/8UQbq+qTVfUMuutUdI8ix/z6viR5CPBIpr5+D7gnK/Ge0VrK0CONqCRbJnlekofRzb+5g+5xAXTzdR6Z5KEDh2xEN1pwV5KnAH8yg9N9BPiHJNuls0uShwNfBB6X5KVJ1mmvJyd5fJKHpptYvaA9nrhtoH/j/Tvw+iSPTLIpcNgkdf+fJI9sH95M901usO5Hz6CuMX/Zzr0Z8BZgbKLrhcBOSXZLN7l58bjjpjrfycBbk2yRZHPg7cBJk+w/oaq6im4+0LuTrJdkF+AVwCdm2lYz7evURliOB96X5BEASbZO8pyJ9k/ywiTbJ3lIki3oHkud30Z9xu+7fZLfTze5+y66cDX4HnlSa28+3YjQ3XRzpCbzc7pHmr+ub4r3jHrE0CONrocAf033k+8v6Obn/EXb9p/AJcB1SW5s6/4CeEeS2+m++Q4bdZnIe9v+Z9KFl48C67fHLc+mm99xDd0jlrFJw9DNOVme5Da6x2l/OqT944Gv0oWM84BTJ+nLk4H/SnIH3cjVX1XVlW3bYuDE9hjmJcMamMAnW21XtNeRAFX1Y7rJvV+n+9Xr8XNBPko3H+WWJKdN0O6RwBLgIrqJ4OeNtb0S/phuku41wOfo5il9bSXbWszMrtObgcuAc9u9/DoTzD1qtqZ7/HU7Xc33AQcM2Xdd4Ci60aTr6EaE3jKw/fN0j3BvpnsvvbAF6KHaI+B3At9u9T2Vyd8z6pFUTWfkV5KkNSfdn1R4bFUNC8rSjDnSI0mSesHQI0mSesHHW5IkqRcc6ZEkSb3gf763Ftl8881r0aJFs90NSZLWiKVLl95YVVP+ocwxhp61yKJFi1iyZMlsd0OSpDUiyWR/vf0BfLwlSZJ6wdAjSZJ6wdAjSZJ6wdAjSZJ6wdAjSZJ6wdAjSZJ6wdAjSZJ6wdAjSZJ6wT9OuBZZdvWtLDrsjNnuhkbA8qP2n+0uSNIa50iPJEnqBUOPJEnqBUOPJEnqBUOPJEnqBUOPJEnqBUOPJEnqBUOPJEnqBUOPJEnqBUOPJEnqBUOPJEnqBUOPJEnqBUOPJEnqBUOPJEnqBUOPJEnqBUOPJEnqBUOPJEnqBUOPJEnqBUOPJEnqBUOPJEnqBUOPJEnqBUOPJEnqBUOPJEnqBUOPJEnqBUOPJEnqhTkZepI8PMkF7XVdkqsHPn7oNI7fK8nvDtn2vCSHrWS/3rIyx0mSpNk3f7Y7MJGqugnYDSDJYuCOqjp6Bk3sBdwBfGeCtk8HTl/Jrr0FeNdKHitJkmbRnBzpmUiSJyU5J8nSJF9NsrCtf32SHyS5KMmnkiwCXgO8oY0M7TmunZcleX9bPiHJsUm+k+SKJC9u6xcm+UY7/uIkeyY5Cli/rftE2++01p9Lkrx64Bx3JHlnkguTnJtky7Z+yySfa+svHBuNSvKnSb7X2v6XJPPa64R2/mVJ3rD6r7IkSWuvOTnSM4EA/ww8v6p+nuRA4J3Ay4HDgEdV1d1JNqmqW5J8mOmPDi0EngHsQDcC9FngT4CvVtU7k8wDNqiqbyZ5XVXtNnDsy6vqF0nWB76f5JQ2SvUw4NyqOjzJPwKvAo4EjgXOqaoDWrsbJnk8cCDw9KpakeSDwMHAJcDWVfUEgCSbTHhhurD1aoB5G28xnWspSVIvjUroWRd4AvC1JADzgGvbtouATyQ5DThtJdo+raruA34wNiIDfB/4WJJ12vYLhhz7+iQHtOVtgO2Am4D/Ab7Y1i8F/qAt/z7wZwBVdS9wa5KXAk+iC00A6wM3AF8AHp3kn4EzgDMn6kBVHQccB7Duwu1qZqVLktQfo/J4K8AlVbVbe+1cVc9u2/YHPkAXHJYmmWmQu3vceaiqbwDPBK4G/i3Jnz2gQ8lewD7A06pqV+B8YL22eUVVjQWQe5k8XAY4caC27atqcVXdDOwKnA38JfCRGdYlSZIGjErouRvYIsnTAJKsk2SnJA8Btqmqs4C/BTYBNgRuBzZa2ZMl2Ra4oaqOBz4K7N42rWijPwALgJur6s4kOwBPnUbT/wG8tp1jXpKN27oXJ3lEW79Zkm2TbA48pKpOAd420AdJkrQSRuXx1n3Ai4Fjkyyg6/cxwI+Bk9q6AO9rc3q+AHw2yfOB/1dV35zh+fYC/ibJCrrfAhsb6TkOuCjJeXTziV6T5CLgUuDcabT7V8BxSV5BNwL02qr6bpK3Ame2ELeCbmTnV8DH2zqAv5thDZIkaUB+8xRGo27dhdvVwkOOme1uaAQsP2r/2e6CJD1oSZZW1R7T3X9UHm9JkiQ9KIYeSZLUC4YeSZLUC4YeSZLUC4YeSZLUC4YeSZLUC4YeSZLUC4YeSZLUC4YeSZLUC4YeSZLUC4YeSZLUC4YeSZLUC4YeSZLUC4YeSZLUC4YeSZLUC4YeSZLUC4YeSZLUC4YeSZLUC4YeSZLUC4YeSZLUC4YeSZLUC4YeSZLUC4YeSZLUC/NnuwNadXbeegFLjtp/trshSdKc5EiPJEnqBUOPJEnqBUOPJEnqBUOPJEnqBUOPJEnqBUOPJEnqBUOPJEnqBUOPJEnqBUOPJEnqBUOPJEnqBUOPJEnqBUOPJEnqBUOPJEnqBf+X9bXIsqtvZdFhZ8x2NyStBZYftf9sd0Fa5RzpkSRJvWDokSRJvWDokSRJvWDokSRJvWDokSRJvWDokSRJvWDokSRJvWDokSRJvWDokSRJvWDokSRJvWDokSRJvWDokSRJvWDokSRJvWDokSRJvWDokSRJvWDokSRJvWDokSRJvWDokSRJvWDokSRJvWDokSRJvWDokSRJvWDokSRJvWDokSRJvWDokSRJvTBp6ElydpLnjFt3aJIPTnHMHm35S0k2mWCfxUneNMW5X5Bkx4GP35Fkn8mOmY4keyQ5diWPPTTJBg+2D5Ikac2baqTnZOCgcesOauunVFX7VdUtK9EvgBcAvw49VfX2qvr6SrY12KclVfX6lTz8UMDQI0nSCJoq9HwW+MMk6wIkWQRsBXwryYeSLElySZIjJjo4yfIkm7flw5NcmuTrwPYD+7wqyfeTXJjklCQbJPld4HnAPyW5IMljkpyQ5MXtmGclOT/JsiQfG+jf8iRHJDmvbdthgj7tleSLbXlxO/7sJFckeX1b/7AkZ7Q+XZzkwLZtK+CsJGe1/Sa8BsP6kWTDJB9v6y5K8qK2/tlJvtv2/0ySDdv6o5L8oO179BT3SpIkTWLS0FNVNwHfA57bVh0EfLqqCji8qvYAdgF+L8kuw9pJ8qR27BOBFwJPHth8alU9uap2BX4IvKKqvgOcDvxNVe1WVZcPtLUecAJwYFXtDMwHXjvQ3o1VtTvwIWDSR2jNDsBzgKcAf59knVbvNVW1a1U9AfhKVR0LXAPsXVV7t2MnuwYT9eNtwK1VtXNV7QL8ZwuFbwX2afsvAd6YZDPgAGCntu+RE3U+yatb8Fpy7523TqNcSZL6aToTmQcfcQ0+2npJkvOA84GdGHgUNYE9gc9V1Z1VdRtdoBnzhCTfTLIMOLi1NZntgSur6sft4xOBZw5sP7X9uxRYNEVbAGdU1d1VdSNwA7AlsAzYJ8l7kuxZVcPSxGTXYKJ+7AN8YGyHqroZeGo77ttJLgAOAbYFbgPuAj6S5IXAnRN1oKqOq6o9qmqPeRssmEa5kiT103RCz2nAs5LsDqxfVecleRTd6MWz2ijEGcB6U7RTQ9afALyujdocMY12MsX2u9u/99KNAk3l7oHle4H5LVA9iS78vDvJ2x/QiamvwUT9CA+8DgG+1ka0dquqHavqFVV1D93o0yl085u+Mo1aJEnSEFOGnqq6Azgb+Bi/GeXZGPglcGuSLYF9p2jmG8ABSdZPshHwRwPbNgKubY+VDh5Yf3vbNt6PgEVJHts+filwzlR1zESSrYA7q+ok4Ghg9wn6NNNrAHAm8LqB82wKnAs8fayeNqfpcW1ez4Kq+hLdBOrdHmxdkiT12XRGQqALO6fSHnNV1YVJzgcuAa4Avj3ZwW106NPABcBPgW8ObH4b8F9t/TJ+Eyo+BRzfJhC/eKCtu5L8OfCZJPOB7wMfnmYd07Uz3STq+4AV/GbO0HHAl5NcW1V7z+QaNEcCH0hyMd0I0BFVdWqSlwEnj03Ippvjczvw+TaHKcAbVlFtkiT1Uro5yVobrLtwu1p4yDGz3Q1Ja4HlR+0/212QppRkafuFomnxLzJLkqReMPRIkqReMPRIkqReMPRIkqReMPRIkqReMPRIkqReMPRIkqReMPRIkqReMPRIkqReMPRIkqReMPRIkqReMPRIkqReMPRIkqReMPRIkqReMPRIkqReMPRIkqReMPRIkqReMPRIkqReMPRIkqReMPRIkqReMPRIkqReMPRIkqRemD/bHdCqs/PWC1hy1P6z3Q1JkuYkR3okSVIvGHokSVIvGHokSVIvGHokSVIvGHokSVIvGHokSVIvGHokSVIvGHokSVIvGHokSVIvGHokSVIvGHokSVIvGHokSVIvGHokSVIv+L+sr0WWXX0riw47Y7a7IUnSAyw/av/Z7oIjPZIkqR8MPZIkqRcMPZIkqRcMPZIkqRcMPZIkqRcMPZIkqRcMPZIkqRcMPZIkqRcMPZIkqRcMPZIkqRcMPZIkqRcMPZIkqRcMPZIkqRcMPZIkqRcMPZIkqRcMPZIkqRcMPZIkqRcMPZIkqRcMPZIkqRcMPZIkqRcMPZIkqRcMPZIkqRcMPZIkqRcMPZIkqRcMPZIkqRdWSehJ8vAkF7TXdUmuHvj4oVMcu0eSY6dxju+sor5O63xDjj00yQaroh+SJGnNmr8qGqmqm4DdAJIsBu6oqqPHtieZX1X3DDl2CbBkGuf43VXU12mdb4hDgZOAO1dFXyRJ0pqz2h5vJTkhyXuTnAW8J8lTknwnyfnt3+3bfnsl+WJbXpzkY0nOTnJFktcPtHfHwP5nJ/lskh8l+USStG37tXXfSnLsWLvj+jXl+ZI8LMkZSS5McnGSA9u2rYCzWk0k+VCSJUkuSXLEwDmWJzkiyXlJliXZoa3fMMnH27qLkryorX92ku+2/T+TZMO2/qgkP2j7Ho0kSVppq2SkZxKPA/apqnuTbAw8s6ruSbIP8C7gRRMcswOwN7ARcGmSD1XVinH7PBHYCbgG+Dbw9CRLgH9p57gyycnT7OMDzgc8F7imqvYHSLKgqm5N8kZg76q6sR17eFX9Isk84D+S7FJVF7VtN1bV7kn+AngT8ErgbcCtVbVza3fTJJsDb23X6ZdJ3gy8Mcn7gQOAHaqqkmwyUeeTvBp4NcC8jbeYZsmSJPXP6p7I/JmqurctLwA+k+Ri4H10oWUiZ1TV3S1Y3ABsOcE+36uqn1XVfcAFwCK68HJFVV3Z9plu6JnofMuAfZK8J8meVXXrkGNfkuQ84PxWz44D205t/y5t/QPYB/jA2A5VdTPw1Hbct5NcABwCbAvcBtwFfCTJCxnySK2qjquqPapqj3kbLJhmyZIk9c/qDj2/HFj+B+CsqnoC8EfAekOOuXtg+V4mHo2aaJ+sZB8f0FZV/Rh4El34eXeSt48/KMmj6EZwnlVVuwBncP+axtodrCFAjW8K+FpV7dZeO1bVK9ocqKcApwAvAL6ykvVJkiTW7K+sLwCubssvWw3t/wh4dJJF7eMDV7ahJFsBd1bVScDRwO5t0+10j8EANqYLdbcm2RLYdxpNnwm8buA8mwLn0j2ee2xbt0GSx7V5PQuq6kt0E6h3W9l6JEnS6p/TM+gfgRPbvJj/XNWNV9Wv2vyZryS5Efjeg2huZ+CfktwHrABe29YfB3w5ybVVtXeS84FLgCvo5hZN5UjgA+0R373AEVV1apKXAScnWbft91a6gPX5JOvRjQa94UHUI0lS76Vq/NOW0ZVkw6q6o/021weAn1TV+2a7X2vKugu3q4WHHDPb3ZAk6QGWH7X/Km8zydKq2mO6+69tf5H5VW0y8CV0j9P+ZXa7I0mS5oo1+XhrtWujOr0Z2ZEkSdO3to30SJIkTcjQI0mSesHQI0mSesHQI0mSesHQI0mSesHQI0mSesHQI0mSesHQI0mSesHQI0mSesHQI0mSesHQI0mSesHQI0mSesHQI0mSesHQI0mSesHQI0mSesHQI0mSesHQI0mSesHQI0mSesHQI0mSesHQI0mSemH+bHdAq87OWy9gyVH7z3Y3JEmakxzpkSRJvWDokSRJvWDokSRJvWDokSRJvWDokSRJvWDokSRJvWDokSRJvWDokSRJvWDokSRJvZCqmu0+aBVJcjtw6Wz3YzXZHLhxtjuxGqytdYG1jSprG019rW3bqtpiug3531CsXS6tqj1muxOrQ5Ila2Nta2tdYG2jytpGk7VNj4+3JElSLxh6JElSLxh61i7HzXYHVqO1tba1tS6wtlFlbaPJ2qbBicySJKkXHOmRJEm9YOiRJEm9YOhZCyR5bpJLk1yW5LDZ7s9MJdkmyVlJfpjkkiR/1dYvTnJ1kgvaa7+BY/6u1XtpkufMXu+nlmR5kmWthiVt3WZJvpbkJ+3fTQf2H4nakmw/cG8uSHJbkkNH9b4l+ViSG5JcPLBuxvcpyZPa/b4sybFJsqZrGW9Ibf+U5EdJLkryuSSbtPWLkvxq4P59eOCYOVXbkLpm/P6ba3W1Pk1U26cH6lqe5IK2fmTuWevTsK/5q//zrap8jfALmAdcDjwaeChwIbDjbPdrhjUsBHZvyxsBPwZ2BBYDb5pg/x1bnesCj2r1z5vtOiapbzmw+bh1/wgc1pYPA94zirUN1DMPuA7YdlTvG/BMYHfg4gdzn4DvAU8DAnwZ2HeO1vZsYH5bfs9AbYsG9xvXzpyqbUhdM37/zbW6htU2bvv/B94+aves9WnY1/zV/vnmSM/oewpwWVVdUVX/A3wKeP4s92lGquraqjqvLd8O/BDYepJDng98qqrurqorgcvorsMoeT5wYls+EXjBwPpRrO1ZwOVV9dNJ9pnTtVXVN4BfjFs9o/uUZCGwcVV9t7qvyP86cMysmai2qjqzqu5pH54LPHKyNuZibUPu2TAjf8/GtNGMlwAnT9bGHK5t2Nf81f75ZugZfVsDVw18/DMmDwxzWpJFwBOB/2qrXteG3z82MNQ5ajUXcGaSpUle3dZtWVXXQvcFAHhEWz9qtY05iPt/AV4b7hvM/D5t3ZbHr5/rXk73U/KYRyU5P8k5SfZs60aptpm8/0aprjF7AtdX1U8G1o3kPRv3NX+1f74ZekbfRM8vR/LvECTZEDgFOLSqbgM+BDwG2A24lm44F0av5qdX1e7AvsBfJnnmJPuOWm0keSjwPOAzbdXact8mM6yWkasxyeHAPcAn2qprgd+uqicCbwQ+mWRjRqe2mb7/RqWuQX/M/X/IGMl7NsHX/KG7TrBupe6doWf0/QzYZuDjRwLXzFJfVlqSdeje/J+oqlMBqur6qrq3qu4Djuc3j0JGquaquqb9ewPwObo6rm9Ds2ND0De03UeqtmZf4Lyquh7WnvvWzPQ+/Yz7Pyaa0zUmOQT4Q+Dg9niA9gjhpra8lG7+xOMYkdpW4v03EnWNSTIfeCHw6bF1o3jPJvqazxr4fDP0jL7vA9sleVT7ifsg4PRZ7tOMtOfTHwV+WFXvHVi/cGC3A4Cx32I4HTgoybpJHgVsRzeZbc5J8rAkG40t000evZiuhkPabocAn2/LI1PbgPv91Lk23LcBM7pPbUj+9iRPbe/rPxs4Zk5J8lzgzcDzqurOgfVbJJnXlh9NV9sVo1LbTN9/o1LXgH2AH1XVrx/rjNo9G/Y1nzXx+Tbbs7h9rZKZ8PvRzX6/HDh8tvuzEv1/Bt2Q5EXABe21H/BvwLK2/nRg4cAxh7d6L2UO/DbCJLU9mu63Di4ELhm7P8DDgf8AftL+3WzUamt93QC4CVgwsG4k7xtdcLsWWEH3E+QrVuY+AXvQfaO9HHg/7S/fz8HaLqObJzH2Offhtu+L2nv1QuA84I/mam1D6prx+2+u1TWstrb+BOA14/YdmXvW+jTsa/5q/3zzv6GQJEm94OMtSZLUC4YeSZLUC4YeSZLUC4YeSZLUC4YeSZLUC4YeSZLUC4YeSZLUC/8L8bhb+B60wLkAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 576x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_instances(train_size, val_size, test_size):\n",
    "    x_values = [train_size, val_size, test_size]\n",
    "    y_values = [\"Training instances\", \"Validation instances\", \"Test instances\"]\n",
    "    y_axis = np.arange(1, 4, 1)\n",
    "    plt.figure(figsize=(8,4))\n",
    "    plt.title(\"Instances distribution on the 3 splits\")\n",
    "    plt.barh(y_axis, x_values, align='center')\n",
    "    plt.yticks(y_axis, y_values)\n",
    "    plt.show()\n",
    "\n",
    "train_size = len(train_sentences)\n",
    "val_size = len(val_sentences)\n",
    "test_size = len(test_sentences)\n",
    "print(\"There are {} training sentences\".format(train_size))\n",
    "print(\"There are {} validation sentences\".format(val_size))\n",
    "print(\"There are {} test sentences\".format(test_size))\n",
    "\n",
    "plot_instances(train_size, val_size, test_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compute the average sentence length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average sentence length in the training set is 24.45919 with an std of 12.68342 and a max of 250\n",
      "The average sentence length in the validation set is 24.82723 with an std of 11.93318 and a max of 88\n",
      "The average sentence length in the test set is 24.44182 with an std of 10.02017 and a max of 80\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 504x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "len_train = [len(sentence) for sentence in train_sentences]\n",
    "len_val   = [len(sentence) for sentence in val_sentences  ]\n",
    "len_test  = [len(sentence) for sentence in test_sentences ]\n",
    "\n",
    "mean_train, std_train, max_train = np.mean(len_train), np.std(len_train), np.max(len_train)\n",
    "mean_val, std_val, max_val = np.mean(len_val), np.std(len_val), np.max(len_val)\n",
    "mean_test, std_test, max_test = np.mean(len_test), np.std(len_test), np.max(len_test)\n",
    "\n",
    "print(\"The average sentence length in the training set is {:.5f} with an std of {:.5f} and a max of {}\".format(\n",
    "    mean_train, std_train, max_train\n",
    "))\n",
    "print(\"The average sentence length in the validation set is {:.5f} with an std of {:.5f} and a max of {}\".format(\n",
    "    mean_val, std_val, max_val\n",
    "))\n",
    "print(\"The average sentence length in the test set is {:.5f} with an std of {:.5f} and a max of {}\".format(\n",
    "    mean_test, std_test, max_test\n",
    "))\n",
    "\n",
    "len_dict = {'Training sentences':    len_train, \n",
    "            'Validation sentences':  len_val, \n",
    "            'Test sentences':        len_test}\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7,5));\n",
    "ax.boxplot(len_dict.values());\n",
    "ax.set_xticklabels(len_dict.keys());\n",
    "ax.set_ylim(0, 120)\n",
    "ax.set_ylabel(\"Sentence lengths\")\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sentences seem to have approximately the same distribution between the three splits, but there are some clear outliers in the training set.\n",
    "\n",
    "We also compute the distribution of the tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tag</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>#</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>$</th>\n",
       "      <td>342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>''</th>\n",
       "      <td>398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>,</th>\n",
       "      <td>2571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-LRB-</th>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-RRB-</th>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.</th>\n",
       "      <td>1959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>:</th>\n",
       "      <td>293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CC</th>\n",
       "      <td>1141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CD</th>\n",
       "      <td>1439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DT</th>\n",
       "      <td>4076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EX</th>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FW</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IN</th>\n",
       "      <td>4952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>JJ</th>\n",
       "      <td>2992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>JJR</th>\n",
       "      <td>157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>JJS</th>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LS</th>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MD</th>\n",
       "      <td>413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NN</th>\n",
       "      <td>6270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NNP</th>\n",
       "      <td>5200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NNPS</th>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NNS</th>\n",
       "      <td>3004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PDT</th>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>POS</th>\n",
       "      <td>403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PRP</th>\n",
       "      <td>954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PRP$</th>\n",
       "      <td>409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RB</th>\n",
       "      <td>1490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RBR</th>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RBS</th>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RP</th>\n",
       "      <td>140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SYM</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TO</th>\n",
       "      <td>1028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>UH</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VB</th>\n",
       "      <td>1195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VBD</th>\n",
       "      <td>1548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VBG</th>\n",
       "      <td>763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VBN</th>\n",
       "      <td>1031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VBP</th>\n",
       "      <td>727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VBZ</th>\n",
       "      <td>1133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WDT</th>\n",
       "      <td>204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WP</th>\n",
       "      <td>141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WP$</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WRB</th>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>``</th>\n",
       "      <td>409</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       word\n",
       "tag        \n",
       "#         1\n",
       "$       342\n",
       "''      398\n",
       ",      2571\n",
       "-LRB-    52\n",
       "-RRB-    55\n",
       ".      1959\n",
       ":       293\n",
       "CC     1141\n",
       "CD     1439\n",
       "DT     4076\n",
       "EX       49\n",
       "FW        2\n",
       "IN     4952\n",
       "JJ     2992\n",
       "JJR     157\n",
       "JJS      93\n",
       "LS       10\n",
       "MD      413\n",
       "NN     6270\n",
       "NNP    5200\n",
       "NNPS     95\n",
       "NNS    3004\n",
       "PDT       9\n",
       "POS     403\n",
       "PRP     954\n",
       "PRP$    409\n",
       "RB     1490\n",
       "RBR      86\n",
       "RBS      19\n",
       "RP      140\n",
       "SYM       1\n",
       "TO     1028\n",
       "UH        1\n",
       "VB     1195\n",
       "VBD    1548\n",
       "VBG     763\n",
       "VBN    1031\n",
       "VBP     727\n",
       "VBZ    1133\n",
       "WDT     204\n",
       "WP      141\n",
       "WP$       6\n",
       "WRB      92\n",
       "``      409"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tags_distribution = pd.concat(train_sentences, axis=0).groupby(['tag']).count()\n",
    "display(tags_distribution)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: see [here](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html) for the explanation of each tag. Some of the tags simply indicate punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 6081 punctuation tags in the dataset\n"
     ]
    }
   ],
   "source": [
    "PUNCTUATION_TAGS = [\"#\", \"$\", \"''\", \",\", \".\", \":\", \"``\", \"-LRB-\", \"-RRB-\", \"SYM\"]\n",
    "print(\"There are {} punctuation tags in the dataset\".format(\n",
    "    sum([ tags_distribution.loc[pun] for pun in PUNCTUATION_TAGS ])['word']\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this task, we don't want to remove words because we should provide a one-to-one mapping from words to tags.\n",
    "\n",
    "Therefore, the only sensible preprocessing we can do on the sentences is to bring everything to lowercase.\n",
    "\n",
    "TODO!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>mr.</td>\n",
       "      <td>NNP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>vinken</td>\n",
       "      <td>NNP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>is</td>\n",
       "      <td>VBZ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>chairman</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>of</td>\n",
       "      <td>IN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>elsevier</td>\n",
       "      <td>NNP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>n.v.</td>\n",
       "      <td>NNP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>,</td>\n",
       "      <td>,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>the</td>\n",
       "      <td>DT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>dutch</td>\n",
       "      <td>NNP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>publishing</td>\n",
       "      <td>VBG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>group</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          word  tag\n",
       "18         mr.  NNP\n",
       "19      vinken  NNP\n",
       "20          is  VBZ\n",
       "21    chairman   NN\n",
       "22          of   IN\n",
       "23    elsevier  NNP\n",
       "24        n.v.  NNP\n",
       "25           ,    ,\n",
       "26         the   DT\n",
       "27       dutch  NNP\n",
       "28  publishing  VBG\n",
       "29       group   NN\n",
       "30           .    ."
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def to_lower(word: str):\n",
    "    return word.lower()\n",
    "\n",
    "for sentence_group in [train_sentences, val_sentences, test_sentences]:\n",
    "    for sentence in sentence_group:\n",
    "        sentence['word'] = sentence['word'].apply(to_lower)\n",
    "\n",
    "train_sentences[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We download GloVe's pretrained embeddings with dimension 100 using the Gensim library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embedding_model(embedding_dimension: int = 50) -> gensim.models.keyedvectors.KeyedVectors:\n",
    "    \"\"\"\n",
    "    Loads GloVe pre-trained word embedding model via gensim library.\n",
    "\n",
    "    Parameters:\n",
    "    - embedding_dimension: size of the embedding space to consider\n",
    "\n",
    "    Returns:\n",
    "        - pre-trained word embedding model (gensim KeyedVectors object)\n",
    "    \"\"\"\n",
    "    assert embedding_dimension in {50,100,200,300}, \"Embedding dimension must be one of 50,100,200,300\"\n",
    "    download_path = \"glove-wiki-gigaword-{}\".format(embedding_dimension)\n",
    "    try:\n",
    "        emb_model = gloader.load(download_path)\n",
    "    except:\n",
    "        print(\"ERROR: No Internet connection available\")\n",
    "        return None\n",
    "    return emb_model\n",
    "\n",
    "EMBEDDING_DIMENSION = 100\n",
    "embedding_model = load_embedding_model(EMBEDDING_DIMENSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GloVe's vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use as initial vocabulary the vocabulary of GloVe's pretrained embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting vocabulary contains 400000 words.\n"
     ]
    }
   ],
   "source": [
    "embedding_vocabulary = set(embedding_model.index_to_key)\n",
    "print(\"Starting vocabulary contains {} words.\".format(len(embedding_vocabulary)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The vocabulary of the embeddings may not contain all the words that are used in our sentences, so we need to create sensible encodings for OOV words and add them to the vocabulary.\n",
    "\n",
    "A smart idea to create OOV embeddings is to obtain the mean and standard deviation of the GloVe embeddings and to sample new random vectors from a reduced uniform distribution with those parameters. In this way, we make sure that OOV embeddings are at least similar to the real embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 400000/400000 [00:04<00:00, 94746.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean embedding vector: 0.004451991990208626\n",
      "Std of embedding vectors: 0.4081574082374573\n"
     ]
    }
   ],
   "source": [
    "# Create an empty matrix that should hold all the embeddings of GloVe's vocabulary\n",
    "def create_embedding_matrix(embedding_model: gensim.models.keyedvectors.KeyedVectors):\n",
    "    embedding_vocabulary = set(embedding_model.index_to_key)\n",
    "    embedding_matrix = np.zeros((len(embedding_vocabulary), EMBEDDING_DIMENSION), dtype=np.float32)\n",
    "\n",
    "    # Loop through the words of the vocabulary\n",
    "    for idx, word in tqdm(enumerate(embedding_model.index_to_key), total=len(embedding_vocabulary)):\n",
    "        try:\n",
    "            # Get the vector representation of the word according to the embedding model\n",
    "            embedding_vector = embedding_model[word]\n",
    "            # Set the corresponding row to the embedding vector\n",
    "            embedding_matrix[idx] = embedding_vector\n",
    "        except (KeyError, TypeError):\n",
    "            pass\n",
    "    \n",
    "    return embedding_matrix\n",
    "\n",
    "embedding_matrix = create_embedding_matrix(embedding_model)\n",
    "\n",
    "# Calculate mean and standard deviation\n",
    "glove_mean = embedding_matrix.mean()\n",
    "glove_std = embedding_matrix.std()\n",
    "\n",
    "print(\"Mean embedding vector: {}\".format(glove_mean))\n",
    "print(\"Std of embedding vectors: {}\".format(glove_std))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can define a function to create random embeddings for OOV words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_embedding():\n",
    "    return np.random.uniform(low=glove_mean-glove_std/10, \n",
    "        high=glove_mean+glove_std/10, size=EMBEDDING_DIMENSION\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OOV terms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We find all OOV terms in the splits independently and build our vocabularies and embedding matrix. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training OOV terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Obtain a set of words (vocabulary) used in each split\n",
    "def get_set_of_words_for_sentences(sentence_seq: Sequence[pd.DataFrame]):\n",
    "    return set(\n",
    "        pd.concat(sentence_seq, axis=0).    # Concatenate all sentences together\n",
    "        groupby(['word']).                  # Group by words\n",
    "        indices.keys()                      # Obtain the sequence of words\n",
    "    )             \n",
    "\n",
    "train_vocabulary = get_set_of_words_for_sentences(train_sentences)\n",
    "val_vocabulary = get_set_of_words_for_sentences(val_sentences)\n",
    "test_vocabulary = get_set_of_words_for_sentences(test_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG: Training set has 359 OOV terms over 7402 total terms (4.85%)\n",
      "Some OOV terms:\n",
      "['biondi-santi', 'stock-index', 'war-rationed', '37-a-share', '62.625', '449.04', 'bell-ringer', 'uzi-model', 'sogo-shosha', 'electric-utility', 'cotran', 'ntg', '1.457', 'savers\\\\/investors', 'cash-rich', 'custom-chip', 'recession-inspired', '3,288,453', 'index-related', 'when-issued', 'pennview', '705.6', 'foreign-led', 'yen-denominated', 'drobnick', 'ctbs', 'school-board', '382-37', 'replacement-car', 'buttoned-down']\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Find OOV words with respect to some vocabulary\n",
    "def get_OOV_terms(vocab_1: set, vocab_2: set):\n",
    "    return vocab_1.difference(vocab_2)\n",
    "\n",
    "oov_train = get_OOV_terms(train_vocabulary, embedding_vocabulary)\n",
    "\n",
    "print(\"DEBUG: Training set has {} OOV terms over {} total terms ({:.2f}%)\".format(\n",
    "    len(oov_train), len(train_vocabulary), len(oov_train)/len(train_vocabulary)*100\n",
    "))\n",
    "print(\"Some OOV terms:\")\n",
    "print(list(oov_train)[:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OOVs (for the training set) seem to mostly be: \n",
    "- hyphenated compound words\n",
    "- numbers\n",
    "- foreign words\n",
    "- other uncommon words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We update the vocabulary adding these OOV words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New length of the embedding model: 400359\n"
     ]
    }
   ],
   "source": [
    "embedding_model.add_vectors(list(oov_train), \n",
    "    np.stack([get_random_embedding() for _ in range(len(oov_train))])\n",
    ")\n",
    "train_embedding_vocabulary = set(embedding_model.index_to_key)\n",
    "\n",
    "print(\"New length of the embedding model: {}\".format(len(train_embedding_vocabulary)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We repeat the process for the evaluation and test sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validation OOV terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG: Validation set has 189 OOV terms over 5420 total terms (3.49%)\n",
      "Some OOV terms:\n",
      "['pension-fund', 'mutual-fund', 'band-wagon', 'super-absorbent', 'anku', '95.09', 'house-senate', 'million-a-year', 'write-downs', 'melt-textured', '12\\\\/32', 'property\\\\/casualty', 'seven-million-ton', 'foldability', '1738.1', 'diceon', 'gates-warren', 'one-house', 'express-buick', 'price-depressing', 'walbrecher', 'profit-taking', 'scypher', 'anti-program', 'much-larger', 'labor-backed', 'sidak', 'credit-rating', 'motor-home', 'stock-selection']\n",
      "New length of the embedding model: 400548\n"
     ]
    }
   ],
   "source": [
    "# Find OOV words\n",
    "oov_val = get_OOV_terms(val_vocabulary, train_embedding_vocabulary)\n",
    "print(\"DEBUG: Validation set has {} OOV terms over {} total terms ({:.2f}%)\".format(\n",
    "    len(oov_val), len(val_vocabulary), len(oov_val)/len(val_vocabulary)*100\n",
    "))\n",
    "print(\"Some OOV terms:\")\n",
    "print(list(oov_val)[:30])\n",
    "\n",
    "# Update the model\n",
    "embedding_model.add_vectors(list(oov_val), \n",
    "    np.stack([get_random_embedding() for _ in range(len(oov_val))])\n",
    ")\n",
    "val_embedding_vocabulary = set(embedding_model.index_to_key)\n",
    "\n",
    "print(\"New length of the embedding model: {}\".format(len(val_embedding_vocabulary)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test OOV terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG: Test set has 128 OOV terms over 3407 total terms (3.76%)\n",
      "Some OOV terms:\n",
      "['newspaper-printing', '434.4', 'copper-rich', '170,262', 'tete-a-tete', '43.875', 'blue-chips', '372.14', '608,413', 'early-retirement', 'pro-iranian', 'heavy-truck', '1206.26', '142.84', '1.916', 'manmade-fiber', '11,390,000', 'shareholder-rights', 'derchin', 'inter-tel', 'hasbrouk', 'bread-and-butter', '734.9', '83,206', 'yoshihashi', '40-megabyte', 'near-limit', 'intelogic', '38.875', '5,699']\n",
      "New length of the embedding model: 400676\n"
     ]
    }
   ],
   "source": [
    "# Find OOV words\n",
    "oov_test = get_OOV_terms(test_vocabulary, val_embedding_vocabulary)\n",
    "print(\"DEBUG: Test set has {} OOV terms over {} total terms ({:.2f}%)\".format(\n",
    "    len(oov_test), len(test_vocabulary), len(oov_test)/len(test_vocabulary)*100\n",
    "))\n",
    "print(\"Some OOV terms:\")\n",
    "print(list(oov_test)[:30])\n",
    "\n",
    "# Update the model\n",
    "embedding_model.add_vectors(list(oov_test), \n",
    "    np.stack([get_random_embedding() for _ in range(len(oov_test))])\n",
    ")\n",
    "test_embedding_vocabulary = set(embedding_model.index_to_key)\n",
    "\n",
    "print(\"New length of the embedding model: {}\".format(len(test_embedding_vocabulary)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, we notice the same pattern we have exposed for training OOV terms also in the OOV of the other splits. \n",
    "\n",
    "Additionally, we report that the number of OOV terms is always less than 5% in all splits, so they shouldn't be a big problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Padding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's time to think some steps ahead: when we feed our sentences to the model, we usually provide mini-batches of data (for example, a set of 128 sentences at a time). This makes training faster and more stable.\n",
    "\n",
    "A mini-batch of data should be \"rectangular\", meaning that sentences should all have the same number of words. Usually, this translates to using **padding** in the sentences, filling the vectors representing the sentences with zeroes up to a predetermined size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model.add_vector('<PAD>', np.zeros(EMBEDDING_DIMENSION)); # The embedding is a vector of zeroes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should check what the index of the padding vector is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400676"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PADDING_IDX = embedding_model.key_to_index['<PAD>']\n",
    "PADDING_IDX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would like to have the padding token as element 0 (because it can be automatically masked when using [Keras's Embedding Layer](https://keras.io/api/layers/core_layers/embedding/))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final embedding matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 400677/400677 [00:02<00:00, 154921.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG: Embedding matrix shape: (400677, 100)\n"
     ]
    }
   ],
   "source": [
    "embedding_matrix = create_embedding_matrix(embedding_model)\n",
    "# We need to move the row where the padding index is at the first row and shift the rest of the matrix by 1\n",
    "embedding_matrix = np.append(embedding_matrix[-1:, :], embedding_matrix[:-1, :], axis=0)\n",
    "print(\"DEBUG: Embedding matrix shape: {}\".format(embedding_matrix.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a baseline model, we create a network composd by a (non-trainable) Embedding layer, a single Bidirectional LSTM layer and a Time-Distributed Dense layer on top."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset processing pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, though, we should provide some utility functions to facilitate inputting data to the networks.\n",
    "\n",
    "We have decided to use TensorFlow's [Dataset](https://www.tensorflow.org/api_docs/python/tf/data/Dataset) class, since it provides many useful functions for dealing with batches and building efficient data preprocessing pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG: indexes for sentence 'mr. vinken is chairman of elsevier n.v. , the dutch publishing group .':\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1996, 400059, 15, 664, 4, 43651, 60024, 2, 1, 1693, 3649, 130, 3]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A function that converts a sentence to a list of indices indexing the created vocabulary model\n",
    "def sentence_to_idxs(sentence: pd.DataFrame):\n",
    "    sentence_list = get_list_of_words_from_sentence(sentence)\n",
    "    return [ embedding_model.key_to_index[word] + 1 for word in sentence_list ] # +1 for padding\n",
    "\n",
    "print(\"DEBUG: indexes for sentence '{}':\".format(get_natural_language_sentence(train_sentences[1])))\n",
    "sentence_to_idxs(train_sentences[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We apply this function to the entire dataset splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = [ sentence_to_idxs(sentence) for sentence in train_sentences ]\n",
    "X_val   = [ sentence_to_idxs(sentence) for sentence in val_sentences   ]\n",
    "X_test  = [ sentence_to_idxs(sentence) for sentence in test_sentences  ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use [a Tensorflow function](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/sequence/pad_sequences) to automatically apply padding to the sequences.\n",
    "\n",
    "In theory, padding should bring all tensors to have as many elements as the longest sentence. We have analyzed before that the training set has some outliers and the longest sequence contains 250 elements, but bringing all sequences to have 250 elements is clearly a waste of memory.\n",
    "\n",
    "Instead, **only for the training set**, we choose to fix a maximum length similar to the other splits. This will cut a part of the outlier sentences, but since they are only used for training it shouldn't be a big problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80.00034999999934"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.quantile(len_train, 0.99751) # If we choose 80 as maximum length (similar to the other splits), \n",
    "                                # we avoid to cut over 99.75% of the sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = keras.preprocessing.sequence.pad_sequences(\n",
    "    X_train, maxlen=80, truncating='post'   # truncating='post' means that we cut the last part of the sentence\n",
    ")\n",
    "\n",
    "X_val = keras.preprocessing.sequence.pad_sequences(X_val) # No cutting\n",
    "X_test = keras.preprocessing.sequence.pad_sequences(X_test) # No cutting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO: We also need to encode labels and add them to the datasets below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices(X_train).batch(BATCH_SIZE)\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices(X_val).batch(BATCH_SIZE)\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices(X_test).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error Analysis"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c90f87e5ee49fb7af5481158977e62ba01ae4f54defb58032c9ba197b530ea3c"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('NLP': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
